{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import joblib\n","import h5py\n","import io\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","from tensorflow.keras.utils import Sequence\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.applications.resnet50 import preprocess_input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate\n","from tensorflow.keras.applications import ResNet50\n","\n","from PIL import Image\n","import warnings\n","\n","# Suppress all warnings\n","warnings.filterwarnings('ignore')\n","class DataPreprocessor:\n","\n","    def __init__(self,numerical_features,categorical_features):\n","        self.numerical_features = numerical_features\n","        self.categorical_features = categorical_features\n","\n","    def custom_preprocessing(self, data):\n","        \n","        data.dropna(inplace=True)\n","        scaler = StandardScaler()\n","        data[numerical_features] = scaler.fit_transform(data[numerical_features])\n","        \n","        encoder = OneHotEncoder(sparse=False)\n","        encoded_categories = encoder.fit_transform(data[categorical_features])\n","        \n","        encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_features))\n","        data = pd.concat([data.drop(columns=categorical_features), encoded_df], axis=1)\n","        \n","        return data\n","\n","    def fit_transform(self, data):\n","        data_preprocessed = self.custom_preprocessing(data)\n","        return data_preprocessed\n","class ImageDataGenerator(Sequence):\n","    def __init__(self, hdf5_file, metadata_df, batch_size=32, image_size=(128, 128)):\n","        self.hdf5_file = h5py.File(hdf5_file, 'r')\n","        self.metadata_df = metadata_df\n","        self.batch_size = batch_size\n","        self.image_size = image_size\n","        self.indexes = np.arange(len(self.metadata_df))\n","        \n","\n","    def __len__(self):\n","        return int(np.floor(len(self.metadata_df) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        batch_start = index * self.batch_size\n","        batch_end = min(batch_start + self.batch_size, len(self.metadata_df))\n","        batch_indexes = self.indexes[batch_start:batch_end]\n","        batch_metadata = self.metadata_df.iloc[batch_indexes]\n","        images, metadata = self.__data_generation(batch_metadata)\n","        \n","        return (images, metadata), batch_metadata['target'].values\n","\n","\n","    def __data_generation(self, batch_metadata):\n","        images = []\n","        metadata = []\n","        isic_ids = batch_metadata['isic_id'].values\n","        batch_metadata = batch_metadata.drop(columns=['isic_id','target'])\n","        for i, row in enumerate(batch_metadata.iterrows()):\n","            isic_id = isic_ids[i]\n","            try:\n","                # Load and preprocess the image\n","                image_data = self.hdf5_file[isic_id][()]\n","                image = Image.open(io.BytesIO(image_data)).resize(self.image_size)\n","                image = img_to_array(image)\n","                image = preprocess_input(image)\n","                images.append(image)\n","            except Exception as e:\n","                print(f\"Error loading image for isic_id {isic_id}: {e}\")\n","                continue\n","\n","        \n","\n","        \n","            # Convert metadata row to a numpy array\n","            non_image_data = row[1].values  # row[1] gives the Series without the index\n","            metadata.append(non_image_data)\n","\n","        # Convert to numpy arrays\n","        images = np.array(images)\n","        metadata = np.array(metadata)\n","\n","        # Ensure metadata has the correct shape\n","        if metadata.shape[0] != len(images):\n","            raise ValueError(f\"Metadata batch size {metadata.shape[0]} does not match image batch size {len(images)}\")\n","\n","        return images, metadata\n","\n","# Ensure all model layers are initialized in the __init__ method\n","class ModelTrainer:\n","    def __init__(self, image_size, num_features):\n","        self.image_size = image_size\n","        self.num_metadata_features = num_features\n","        self.model = self.build_model()\n","\n","    def build_model(self):\n","        image_input = Input(shape=(self.image_size[0], self.image_size[1], 3))\n","        base_model = ResNet50(weights=None, include_top=False, input_tensor=image_input)\n","        x = Flatten()(base_model.output)\n","\n","        metadata_input = Input(shape=(self.num_metadata_features,))\n","        y = Dense(128, activation='relu')(metadata_input)\n","        y = Dense(64, activation='relu')(y)\n","\n","        combined = Concatenate()([x, y])\n","        z = Dense(128, activation='relu')(combined)\n","        z = Dense(64, activation='relu')(z)\n","        output = Dense(1, activation='sigmoid')(z)\n","\n","        model = Model(inputs=[image_input, metadata_input], outputs=output)\n","        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","        return model\n","\n","    def train(self, training_generator, epochs=10):\n","        history = self.model.fit(training_generator, epochs=epochs)\n","        if history and history.history:\n","            print(\"Training history logs:\")\n","            print(history.history)\n","        else:\n","            print(\"Training logs are None or empty!\")\n","\n","        return history\n","train_metadata = pd.read_csv('/kaggle/input/isic-2024-challenge/train-metadata.csv')\n","columns_not_in_test = {'lesion_id','iddx_full','iddx_1','iddx_2','iddx_3','iddx_4','iddx_5','mel_mitotic_index','mel_thick_mm','tbp_lv_dnn_lesion_confidence'}\n","columns_with_missing_values = train_metadata.columns[train_metadata.isnull().sum() > len(train_metadata) * 0.5]\n","train_ids = train_metadata['isic_id']\n","train_targets = train_metadata['target']\n","\n","train_metadata = train_metadata.drop(columns=['isic_id','target'], errors='ignore')\n","train_metadata = train_metadata.drop(columns=columns_with_missing_values, errors='ignore')\n","train_metadata = train_metadata.drop(columns=columns_not_in_test, errors='ignore')\n","numerical_features = train_metadata.select_dtypes(include=['number']).columns.tolist()\n","categorical_features = train_metadata.select_dtypes(include=['object', 'category']).columns.tolist()\n","\n","preprocessor = DataPreprocessor(numerical_features, categorical_features)\n","processed_train_df = preprocessor.fit_transform(train_metadata)\n","processed_train_df = processed_train_df.fillna(processed_train_df.mean())\n","processed_train_df['target'] = train_targets\n","processed_train_df['isic_id'] = train_ids\n","generator = ImageDataGenerator('/kaggle/input/isic-2024-challenge/train-image.hdf5', processed_train_df)\n","num_features = processed_train_df.shape[1] - 2  # Exclude 'isic_id' and 'target'\n","trainer = ModelTrainer(image_size=(128, 128), num_features = num_features)\n","import tensorflow as tf\n","def create_tf_dataset(generator, num_metadata_features, for_prediction=False):\n","    if for_prediction:\n","        output_signature = (\n","            (\n","                tf.TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32),  # Images\n","                tf.TensorSpec(shape=(None, num_metadata_features), dtype=tf.float32)  # Metadata\n","            )\n","        )\n","    else:\n","        output_signature = (\n","            (\n","                tf.TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32),  # Images\n","                tf.TensorSpec(shape=(None, num_metadata_features), dtype=tf.float32)  # Metadata\n","            ),\n","            tf.TensorSpec(shape=(None,), dtype=tf.float32)  # Targets\n","        )\n","\n","    return tf.data.Dataset.from_generator(\n","        lambda: generator,\n","        output_signature=output_signature\n","    )\n","\n","train_dataset = create_tf_dataset(generator, num_features)\n","trainer.train(train_dataset, epochs=3)\n","test_metadata = pd.read_csv('/kaggle/input/isic-2024-challenge/test-metadata.csv')\n","#just for once :\n","test_metadata = pd.read_csv('/kaggle/input/isic-2024-challenge/test-metadata.csv')   \n","test_ids = test_metadata['isic_id']\n","test_metadata = test_metadata.drop(columns=['isic_id'], errors='ignore')\n","test_metadata = test_metadata.drop(columns=columns_with_missing_values, errors='ignore')\n","\n","preprocessor = DataPreprocessor(numerical_features, categorical_features)\n","processed_test_df = preprocessor.fit_transform(test_metadata)\n","\n","columns_to_drop = [col for col in processed_test_df.columns if col not in processed_train_df.columns]\n","processed_test_df = processed_test_df.drop(columns=columns_to_drop)\n","\n","for col in processed_train_df.columns:\n","    if col not in processed_test_df.columns:\n","        processed_test_df[col] = 0\n","\n","\n","processed_test_df = processed_test_df[processed_train_df.columns]\n","processed_test_df['isic_id'] = test_ids\n","processed_test_df['target'] = 0\n","import numpy as np\n","\n","image_input_data = []\n","for isic_id in test_ids:  # Assuming index has isic_ids\n","    image_data = h5py.File('/kaggle/input/isic-2024-challenge/test-image.hdf5', 'r')[isic_id][()]\n","    image = Image.open(io.BytesIO(image_data)).resize((128, 128))\n","    image = img_to_array(image)\n","    image = preprocess_input(image)\n","    image_input_data.append(image)\n","\n","image_input_data = np.array(image_input_data)\n","\n","# Convert the metadata to a NumPy array\n","metadata_input_data = test_features_df.values\n","\n","# Predict using the model\n","predictions = trainer.model.predict([image_input_data, metadata_input_data])\n","\n","# Output predictions\n","print(\"Predictions:\", predictions)\n","\n","threshold = 0.5\n","predicted_labels = (predictions > threshold).astype(int)\n","\n","submission_df = pd.DataFrame({\n","    'isic_id': test_ids,\n","    'target': predicted_labels.flatten()\n","})\n","\n","submission_df.to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":9094797,"sourceId":63056,"sourceType":"competition"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
